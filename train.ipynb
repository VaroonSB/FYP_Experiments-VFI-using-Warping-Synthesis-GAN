{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 1 epoch - 1024 x 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953/953 [==============================] - 7811s 8s/step - loss: 0.0316 - PSNR: 19.9726 - val_loss: 0.0210 - val_PSNR: 22.2682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02099, saving model to ./model_weights\\model_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os.path\n",
    "import h5py\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# from core.createDataset import data_processor\n",
    "from core.imageGenerator import train_generator, valid_generator\n",
    "from core.networks import UNET\n",
    "from core.losses import charbonnier, soft_dice\n",
    "\n",
    "\n",
    "def get_hdf5(filename):\n",
    "    # Reading dataset\n",
    "    hdf5_file = './data/' + filename\n",
    "\n",
    "    # if not os.path.isfile(hdf5_file):\n",
    "    #     data_processor()\n",
    "    return h5py.File(hdf5_file, 'r')\n",
    "\n",
    "def save_history(obj, name):\n",
    "    try:\n",
    "        filename = open(name + \".pickle\",\"wb\")\n",
    "        pickle.dump(obj, filename)\n",
    "        filename.close()\n",
    "        return(True)\n",
    "    except:\n",
    "        return(False)\n",
    "\n",
    "def main():\n",
    "\n",
    "    configs = json.load(open('config.json', 'r'))\n",
    "    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "\n",
    "    hf = get_hdf5(configs['data']['filename'])\n",
    "\n",
    "    x_train = hf.get('x_train')\n",
    "    y_train = hf.get('y_train')\n",
    "    x_valid = hf.get('x_valid')\n",
    "    y_valid = hf.get('y_valid')\n",
    "\n",
    "    nb_epochs = configs['training']['epochs']\n",
    "    batch_size= configs['training']['batch_size']\n",
    "\n",
    "    # Currenty U-net is the only implemented network. By default the size of images is 384x128.\n",
    "    model = UNET((6,512,1024), configs['layer']['activation'])\n",
    "\n",
    "    # We use ADAM optimizer and custom loss functions as 'charbonnier' and 'soft_dice'\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    loss_f    = configs['model']['loss']\n",
    "    if loss_f == \"soft_dice\":\n",
    "        loss = soft_dice\n",
    "    if loss_f == \"charbonnier\":\n",
    "        loss = charbonnier\n",
    "\n",
    "    def PSNR(y_true, y_pred):\n",
    "        max_pixel = 1.0\n",
    "        return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303\n",
    "    # Compile the model. We use accuracy also but it is not so good due to the high number of classes.\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[PSNR])\n",
    "\n",
    "    # We only save the best model and we reduce learning rate when the val_loss is not getting\n",
    "    # better under 10 epoch. It is for SGD.\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(filepath=\"./\" + configs['model']['save_dir'] + \"/\" + configs['model']['file_name'] + \".hdf5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    hist = model.fit_generator(\n",
    "        generator=train_generator(x_train,y_train, batch_size),\n",
    "        steps_per_epoch = x_train.shape[0]/batch_size,\n",
    "        validation_data=valid_generator(x_valid, y_valid, batch_size),\n",
    "        validation_steps= x_valid.shape[0]/batch_size,\n",
    "        epochs = nb_epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    ## Save the history\n",
    "    save_history(hist, \"./\" + configs['model']['save_dir'] + \"/\" +configs['model']['file_name'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 1 epoch 512x265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953/953 [==============================] - 2083s 2s/step - loss: 0.0316 - PSNR: 19.9175 - val_loss: 0.0214 - val_PSNR: 22.2771\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02142, saving model to ./model_weights\\model_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os.path\n",
    "import h5py\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# from core.createDataset import data_processor\n",
    "from core.imageGenerator import train_generator, valid_generator\n",
    "from core.networks import UNET\n",
    "from core.losses import charbonnier, soft_dice\n",
    "\n",
    "\n",
    "def get_hdf5(filename):\n",
    "    # Reading dataset\n",
    "    hdf5_file = './data/' + filename\n",
    "\n",
    "    # if not os.path.isfile(hdf5_file):\n",
    "    #     data_processor()\n",
    "    return h5py.File(hdf5_file, 'r')\n",
    "\n",
    "def save_history(obj, name):\n",
    "    try:\n",
    "        filename = open(name + \".pickle\",\"wb\")\n",
    "        pickle.dump(obj, filename)\n",
    "        filename.close()\n",
    "        return(True)\n",
    "    except:\n",
    "        return(False)\n",
    "\n",
    "def main():\n",
    "\n",
    "    configs = json.load(open('config.json', 'r'))\n",
    "    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "\n",
    "    hf = get_hdf5(configs['data']['filename'])\n",
    "\n",
    "    x_train = hf.get('x_train')\n",
    "    y_train = hf.get('y_train')\n",
    "    x_valid = hf.get('x_valid')\n",
    "    y_valid = hf.get('y_valid')\n",
    "\n",
    "    nb_epochs = configs['training']['epochs']\n",
    "    batch_size= configs['training']['batch_size']\n",
    "\n",
    "    # Currenty U-net is the only implemented network. By default the size of images is 384x128.\n",
    "    model = UNET((6,256,512), configs['layer']['activation'])\n",
    "\n",
    "    # We use ADAM optimizer and custom loss functions as 'charbonnier' and 'soft_dice'\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    loss_f    = configs['model']['loss']\n",
    "    if loss_f == \"soft_dice\":\n",
    "        loss = soft_dice\n",
    "    if loss_f == \"charbonnier\":\n",
    "        loss = charbonnier\n",
    "\n",
    "    def PSNR(y_true, y_pred):\n",
    "        max_pixel = 1.0\n",
    "        return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303\n",
    "    # Compile the model. We use accuracy also but it is not so good due to the high number of classes.\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[PSNR])\n",
    "\n",
    "    # We only save the best model and we reduce learning rate when the val_loss is not getting\n",
    "    # better under 10 epoch. It is for SGD.\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(filepath=\"./\" + configs['model']['save_dir'] + \"/\" + configs['model']['file_name'] + \".hdf5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    hist = model.fit_generator(\n",
    "        generator=train_generator(x_train,y_train, batch_size),\n",
    "        steps_per_epoch = x_train.shape[0]/batch_size,\n",
    "        validation_data=valid_generator(x_valid, y_valid, batch_size),\n",
    "        validation_steps= x_valid.shape[0]/batch_size,\n",
    "        epochs = nb_epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    ## Save the history\n",
    "    save_history(hist, \"./\" + configs['model']['save_dir'] + \"/\" +configs['model']['file_name'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 10 epoch 512x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "953/953 [==============================] - 2182s 2s/step - loss: 0.0305 - PSNR: 20.0406 - val_loss: 0.0208 - val_PSNR: 22.1609\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02080, saving model to ./model_weights\\model_weights.hdf5\n",
      "Epoch 2/5\n",
      "953/953 [==============================] - 2172s 2s/step - loss: 0.0203 - PSNR: 22.0304 - val_loss: 0.0204 - val_PSNR: 22.1980\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02080 to 0.02036, saving model to ./model_weights\\model_weights.hdf5\n",
      "Epoch 3/5\n",
      "953/953 [==============================] - 2157s 2s/step - loss: 0.0189 - PSNR: 22.3287 - val_loss: 0.0687 - val_PSNR: 18.0599\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02036\n",
      "Epoch 4/5\n",
      "953/953 [==============================] - 2139s 2s/step - loss: 0.0184 - PSNR: 22.5008 - val_loss: 0.0214 - val_PSNR: 21.6348\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02036\n",
      "Epoch 5/5\n",
      "953/953 [==============================] - 2142s 2s/step - loss: 0.0174 - PSNR: 22.7657 - val_loss: 0.0187 - val_PSNR: 22.6609\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02036 to 0.01870, saving model to ./model_weights\\model_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os.path\n",
    "import h5py\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# from core.createDataset import data_processor\n",
    "from core.imageGenerator import train_generator, valid_generator\n",
    "from core.networks import UNET\n",
    "from core.losses import charbonnier, soft_dice\n",
    "\n",
    "\n",
    "def get_hdf5(filename):\n",
    "    # Reading dataset\n",
    "    hdf5_file = './data/' + filename\n",
    "\n",
    "    # if not os.path.isfile(hdf5_file):\n",
    "    #     data_processor()\n",
    "    return h5py.File(hdf5_file, 'r')\n",
    "\n",
    "def save_history(obj, name):\n",
    "    try:\n",
    "        filename = open(name + \".pickle\",\"wb\")\n",
    "        pickle.dump(obj, filename)\n",
    "        filename.close()\n",
    "        return(True)\n",
    "    except:\n",
    "        return(False)\n",
    "\n",
    "def main():\n",
    "\n",
    "    configs = json.load(open('config.json', 'r'))\n",
    "    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "\n",
    "    hf = get_hdf5(configs['data']['filename'])\n",
    "\n",
    "    x_train = hf.get('x_train')\n",
    "    y_train = hf.get('y_train')\n",
    "    x_valid = hf.get('x_valid')\n",
    "    y_valid = hf.get('y_valid')\n",
    "\n",
    "    nb_epochs = configs['training']['epochs']\n",
    "    batch_size= configs['training']['batch_size']\n",
    "\n",
    "    # Currenty U-net is the only implemented network. By default the size of images is 384x128.\n",
    "    model = UNET((6,256,512), configs['layer']['activation'])\n",
    "\n",
    "    # We use ADAM optimizer and custom loss functions as 'charbonnier' and 'soft_dice'\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    loss_f    = configs['model']['loss']\n",
    "    if loss_f == \"soft_dice\":\n",
    "        loss = soft_dice\n",
    "    if loss_f == \"charbonnier\":\n",
    "        loss = charbonnier\n",
    "\n",
    "    def PSNR(y_true, y_pred):\n",
    "        max_pixel = 1.0\n",
    "        return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303\n",
    "    # Compile the model. We use accuracy also but it is not so good due to the high number of classes.\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[PSNR])\n",
    "\n",
    "    # We only save the best model and we reduce learning rate when the val_loss is not getting\n",
    "    # better under 10 epoch. It is for SGD.\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(filepath=\"./\" + configs['model']['save_dir'] + \"/\" + configs['model']['file_name'] + \".hdf5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    hist = model.fit_generator(\n",
    "        generator=train_generator(x_train,y_train, batch_size),\n",
    "        steps_per_epoch = x_train.shape[0]/batch_size,\n",
    "        validation_data=valid_generator(x_valid, y_valid, batch_size),\n",
    "        validation_steps= x_valid.shape[0]/batch_size,\n",
    "        epochs = nb_epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    ## Save the history\n",
    "    save_history(hist, \"./\" + configs['model']['save_dir'] + \"/\" +configs['model']['file_name'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple U-Net arch - 512x256 - 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "953/953 [==============================] - 1371s 1s/step - loss: 0.0248 - PSNR: 21.4604 - val_loss: 0.0212 - val_PSNR: 21.4266\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02116, saving model to ./model_weights\\model_weights.hdf5\n",
      "Epoch 2/5\n",
      "953/953 [==============================] - 1351s 1s/step - loss: 0.0191 - PSNR: 22.4564 - val_loss: 0.0194 - val_PSNR: 22.8859\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02116 to 0.01942, saving model to ./model_weights\\model_weights.hdf5\n",
      "Epoch 3/5\n",
      "953/953 [==============================] - 1342s 1s/step - loss: 0.0187 - PSNR: 22.5572 - val_loss: 0.0213 - val_PSNR: 21.4858\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01942\n",
      "Epoch 4/5\n",
      "953/953 [==============================] - 1339s 1s/step - loss: 0.0185 - PSNR: 22.6376 - val_loss: 0.0195 - val_PSNR: 22.6052\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01942\n",
      "Epoch 5/5\n",
      "953/953 [==============================] - 1333s 1s/step - loss: 0.0183 - PSNR: 22.6886 - val_loss: 0.0199 - val_PSNR: 22.3675\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01942\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os.path\n",
    "import h5py\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# from core.createDataset import data_processor\n",
    "from core.imageGenerator import train_generator, valid_generator\n",
    "from core.networks import UNET\n",
    "from core.losses import charbonnier, soft_dice\n",
    "\n",
    "\n",
    "def get_hdf5(filename):\n",
    "    # Reading dataset\n",
    "    hdf5_file = './data/' + filename\n",
    "\n",
    "    # if not os.path.isfile(hdf5_file):\n",
    "    #     data_processor()\n",
    "    return h5py.File(hdf5_file, 'r')\n",
    "\n",
    "def save_history(obj, name):\n",
    "    try:\n",
    "        filename = open(name + \".pickle\",\"wb\")\n",
    "        pickle.dump(obj, filename)\n",
    "        filename.close()\n",
    "        return(True)\n",
    "    except:\n",
    "        return(False)\n",
    "\n",
    "def main():\n",
    "\n",
    "    configs = json.load(open('config.json', 'r'))\n",
    "    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "\n",
    "    hf = get_hdf5(configs['data']['filename'])\n",
    "\n",
    "    x_train = hf.get('x_train')\n",
    "    y_train = hf.get('y_train')\n",
    "    x_valid = hf.get('x_valid')\n",
    "    y_valid = hf.get('y_valid')\n",
    "\n",
    "    nb_epochs = configs['training']['epochs']\n",
    "    batch_size= configs['training']['batch_size']\n",
    "\n",
    "    # Currenty U-net is the only implemented network. By default the size of images is 384x128.\n",
    "    model = UNET((6,256,512), configs['layer']['activation'])\n",
    "\n",
    "    # We use ADAM optimizer and custom loss functions as 'charbonnier' and 'soft_dice'\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    loss_f    = configs['model']['loss']\n",
    "    if loss_f == \"soft_dice\":\n",
    "        loss = soft_dice\n",
    "    if loss_f == \"charbonnier\":\n",
    "        loss = charbonnier\n",
    "\n",
    "    def PSNR(y_true, y_pred):\n",
    "        max_pixel = 1.0\n",
    "        return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303\n",
    "    # Compile the model. We use accuracy also but it is not so good due to the high number of classes.\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[PSNR])\n",
    "\n",
    "    # We only save the best model and we reduce learning rate when the val_loss is not getting\n",
    "    # better under 10 epoch. It is for SGD.\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(filepath=\"./\" + configs['model']['save_dir'] + \"/\" + configs['model']['file_name'] + \".hdf5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    hist = model.fit_generator(\n",
    "        generator=train_generator(x_train,y_train, batch_size),\n",
    "        steps_per_epoch = x_train.shape[0]/batch_size,\n",
    "        validation_data=valid_generator(x_valid, y_valid, batch_size),\n",
    "        validation_steps= x_valid.shape[0]/batch_size,\n",
    "        epochs = nb_epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    ## Save the history\n",
    "    save_history(hist, \"./\" + configs['model']['save_dir'] + \"/\" +configs['model']['file_name'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 405s 2s/step - loss: 0.0579 - PSNR: 17.9659 - val_loss: 0.1163 - val_PSNR: 15.9660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11628, saving model to ./model_weights\\model_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os.path\n",
    "import h5py\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# from core.createDataset import data_processor\n",
    "from core.imageGenerator import train_generator, valid_generator\n",
    "from core.networks import UNET\n",
    "from core.losses import charbonnier, soft_dice\n",
    "\n",
    "\n",
    "def get_hdf5(filename):\n",
    "    # Reading dataset\n",
    "    hdf5_file = './data/' + filename\n",
    "\n",
    "    # if not os.path.isfile(hdf5_file):\n",
    "    #     data_processor()\n",
    "    return h5py.File(hdf5_file, 'r')\n",
    "\n",
    "def save_history(obj, name):\n",
    "    try:\n",
    "        filename = open(name + \".pickle\",\"wb\")\n",
    "        pickle.dump(obj, filename)\n",
    "        filename.close()\n",
    "        return(True)\n",
    "    except:\n",
    "        return(False)\n",
    "\n",
    "def main():\n",
    "\n",
    "    configs = json.load(open('config.json', 'r'))\n",
    "    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n",
    "\n",
    "    hf = get_hdf5(configs['data']['filename'])\n",
    "\n",
    "    x_train = hf.get('x_train')\n",
    "    y_train = hf.get('y_train')\n",
    "    x_valid = hf.get('x_valid')\n",
    "    y_valid = hf.get('y_valid')\n",
    "\n",
    "    nb_epochs = configs['training']['epochs']\n",
    "    batch_size= configs['training']['batch_size']\n",
    "\n",
    "    # Currenty U-net is the only implemented network. By default the size of images is 384x128.\n",
    "    model = UNET((6,256,256), configs['layer']['activation'])\n",
    "\n",
    "    # We use ADAM optimizer and custom loss functions as 'charbonnier' and 'soft_dice'\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    loss_f    = configs['model']['loss']\n",
    "    if loss_f == \"soft_dice\":\n",
    "        loss = soft_dice\n",
    "    if loss_f == \"charbonnier\":\n",
    "        loss = charbonnier\n",
    "\n",
    "    def PSNR(y_true, y_pred):\n",
    "        max_pixel = 1.0\n",
    "        return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303\n",
    "    # Compile the model. We use accuracy also but it is not so good due to the high number of classes.\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[PSNR])\n",
    "\n",
    "    # We only save the best model and we reduce learning rate when the val_loss is not getting\n",
    "    # better under 10 epoch. It is for SGD.\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(filepath=\"./\" + configs['model']['save_dir'] + \"/\" + configs['model']['file_name'] + \".hdf5\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    hist = model.fit_generator(\n",
    "        generator=train_generator(x_train,y_train, batch_size),\n",
    "        steps_per_epoch = x_train.shape[0]/batch_size,\n",
    "        validation_data=valid_generator(x_valid, y_valid, batch_size),\n",
    "        validation_steps= x_valid.shape[0]/batch_size,\n",
    "        epochs = nb_epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    ## Save the history\n",
    "    save_history(hist, \"./\" + configs['model']['save_dir'] + \"/\" +configs['model']['file_name'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "718d2fdbd2e4364d86b82e2681e711484f5b3e9195c217f68eb9a5465be33cfe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('SuperSloMo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
